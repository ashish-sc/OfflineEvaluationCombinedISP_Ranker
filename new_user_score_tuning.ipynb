{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80261a05-ca24-4570-8703-a387ca88e4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import dataclasses\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import cast, Dict, Optional, Sequence, Tuple, Union, List, Text\n",
    "import math\n",
    "from sklearn.metrics import precision_recall_fscore_support,roc_curve,auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow_recommenders as tfrs\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "#from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "logging.getLogger('tensorflow').propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5422a6ea-bba0-4f35-af1f-d00408199451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r434e980225043671_0000018696446de2_1 ... (163s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "!bq extract --noprint_header \\\n",
    "     maximal-furnace-783:Ashish.ranker_isp_nu \\\n",
    "     gs://tpu-cg-us/Ashish/ranker_isp_nu/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cce3810-61d8-4ca3-8988-1d38f557e833",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.get_strategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0fb1dbd-1833-410f-a886-18282409e931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387\n"
     ]
    }
   ],
   "source": [
    "cols = \"isp_date,model,userId,postId,unified_signal1,combined_score,video_play,isp_score,unified_signal,hour,dayofweek,is_weekend,is_morning,is_afternoon,is_evening,is_night,tagId,pvplay_0,pvplay_1,pvplay_2,pvplay_3,pvplay_4,pvplay_5,pvplay_6,pvplay_7,pvplay_8,pvplay_9,pvplay_10,pvplay_11,pvplay_12,pvplay_13,pvplay_14,pvplay_15,pvplay_16,pvplay_17,pvplay_18,pvplay_19,pvplay_20,pvplay_21,pvplay_22,pvplay_23,pvplay_24,pvplay_25,pvplay_26,pvplay_27,pvplay_28,pvplay_29,pvplay_30,pvplay_31,pvplay_mask,pfav_0,pfav_1,pfav_2,pfav_3,pfav_4,pfav_5,pfav_6,pfav_7,pfav_8,pfav_9,pfav_10,pfav_11,pfav_12,pfav_13,pfav_14,pfav_15,pfav_16,pfav_17,pfav_18,pfav_19,pfav_20,pfav_21,pfav_22,pfav_23,pfav_24,pfav_25,pfav_26,pfav_27,pfav_28,pfav_29,pfav_30,pfav_31,pfav_mask,plike_0,plike_1,plike_2,plike_3,plike_4,plike_5,plike_6,plike_7,plike_8,plike_9,plike_10,plike_11,plike_12,plike_13,plike_14,plike_15,plike_16,plike_17,plike_18,plike_19,plike_20,plike_21,plike_22,plike_23,plike_24,plike_25,plike_26,plike_27,plike_28,plike_29,plike_30,plike_31,plike_mask,pshare_0,pshare_1,pshare_2,pshare_3,pshare_4,pshare_5,pshare_6,pshare_7,pshare_8,pshare_9,pshare_10,pshare_11,pshare_12,pshare_13,pshare_14,pshare_15,pshare_16,pshare_17,pshare_18,pshare_19,pshare_20,pshare_21,pshare_22,pshare_23,pshare_24,pshare_25,pshare_26,pshare_27,pshare_28,pshare_29,pshare_30,pshare_31,pshare_mask,postLikeRatio2h,postShareRatio2h,postFavRatio2h,postCommentRatio2h,postSVPRatio2h,postLPORatio2h,postLikeRatio1D,postShareRatio1D,postFavRatio1D,postCommentRatio1D,postSVPRatio1D,postLPORatio1D,pcLikeRatio2h,pcShareRatio2h,pcFavRatio2h,pcCommentRatio2h,pcSVPRatio2h,pcLPORatio2h,pcLikeRatio1D,pcShareRatio1D,pcFavRatio1D,pcCommentRatio1D,pcSVPRatio1D,pcLPORatio1D,userDistrict,uvplay_0,uvplay_1,uvplay_2,uvplay_3,uvplay_4,uvplay_5,uvplay_6,uvplay_7,uvplay_8,uvplay_9,uvplay_10,uvplay_11,uvplay_12,uvplay_13,uvplay_14,uvplay_15,uvplay_16,uvplay_17,uvplay_18,uvplay_19,uvplay_20,uvplay_21,uvplay_22,uvplay_23,uvplay_24,uvplay_25,uvplay_26,uvplay_27,uvplay_28,uvplay_29,uvplay_30,uvplay_31,uvplay_mask,ufav_0,ufav_1,ufav_2,ufav_3,ufav_4,ufav_5,ufav_6,ufav_7,ufav_8,ufav_9,ufav_10,ufav_11,ufav_12,ufav_13,ufav_14,ufav_15,ufav_16,ufav_17,ufav_18,ufav_19,ufav_20,ufav_21,ufav_22,ufav_23,ufav_24,ufav_25,ufav_26,ufav_27,ufav_28,ufav_29,ufav_30,ufav_31,ufav_mask,ulike_0,ulike_1,ulike_2,ulike_3,ulike_4,ulike_5,ulike_6,ulike_7,ulike_8,ulike_9,ulike_10,ulike_11,ulike_12,ulike_13,ulike_14,ulike_15,ulike_16,ulike_17,ulike_18,ulike_19,ulike_20,ulike_21,ulike_22,ulike_23,ulike_24,ulike_25,ulike_26,ulike_27,ulike_28,ulike_29,ulike_30,ulike_31,ulike_mask,ushare_0,ushare_1,ushare_2,ushare_3,ushare_4,ushare_5,ushare_6,ushare_7,ushare_8,ushare_9,ushare_10,ushare_11,ushare_12,ushare_13,ushare_14,ushare_15,ushare_16,ushare_17,ushare_18,ushare_19,ushare_20,ushare_21,ushare_22,ushare_23,ushare_24,ushare_25,ushare_26,ushare_27,ushare_28,ushare_29,ushare_30,ushare_31,ushare_mask,video_affinity,userLikeRatio1,userShareRatio1,userFavRatio1,userCommentsRatio1,userSVPRatio1,userLPORatio1,userLikeRatio7,userShareRatio7,userFavRatio7,userCommentsRatio7,userSVPRatio7,userLPORatio7,upcLikeRatio1D,upcShareRatio1D,upcFavRatio1D,upcCommentRatio1D,upcSVPRatio1D,upcLPORatio1D,upcLikeRatio3D,upcShareRatio3D,upcFavRatio3D,upcCommentRatio3D,upcSVPRatio3D,upcLPORatio3D,upcLikeRatio7D,upcShareRatio7D,upcFavRatio7D,upcCommentRatio7D,upcSVPRatio7D,upcLPORatio7D,engtag_0,engtag_1,engtag_2,engtag_3,engtag_4,engtag_5,engtag_6,engtag_7,engtag_8,engtag_9,engtag_10,engtag_11,engtag_12,engtag_13,engtag_14,engtag_15,engtag_16,engtag_17,engtag_18,engtag_19,engtag_20,engtag_21,engtag_22,engtag_23,engtag_24,engtag_mask_0,engtag_mask_1,engtag_mask_2,engtag_mask_3,engtag_mask_4,engtag_mask_5,engtag_mask_6,engtag_mask_7,engtag_mask_8,engtag_mask_9,engtag_mask_10,engtag_mask_11,engtag_mask_12,engtag_mask_13,engtag_mask_14,engtag_mask_15,engtag_mask_16,engtag_mask_17,engtag_mask_18,engtag_mask_19,engtag_mask_20,engtag_mask_21,engtag_mask_22,engtag_mask_23,engtag_mask_24\"\n",
    "col_names = cols.split(\",\")\n",
    "print(len(col_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dea6abe-3ec3-484b-ac28-f319b8b2f2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 1\n",
    "\n",
    "hour_feat = 1\n",
    "dayofweek = 1\n",
    "num_other_features = 5\n",
    "\n",
    "sparse_features = [\n",
    "    'userDistrict',\n",
    "    'tagId'\n",
    "]\n",
    "\n",
    "max_sequence_length = 25\n",
    "\n",
    "vocab_sizes = {\n",
    "    'userDistrict': 720,\n",
    "    'tagId': 4000\n",
    "}\n",
    "\n",
    "embedding_dims = {\n",
    "    'userDistrict': 32,\n",
    "    'tagId': 32,\n",
    "}\n",
    "\n",
    "meta = [\n",
    "    'isp_date','model','userId','postId','unified_signal1','combined_score','video_play','isp_score'\n",
    "#      'combinedscore','video_play','likes','shares','userId', 'postId', 'L0', 'L1', 'L2'\n",
    "] \n",
    "\n",
    "other_feats = [\n",
    "    'hour', 'dayofweek', 'is_weekend', 'is_morning', 'is_afternoon', 'is_evening', 'is_night'\n",
    "]\n",
    "\n",
    "user_sparse_features = [\n",
    "    'userDistrict'\n",
    "]\n",
    "\n",
    "user_dense_features = [\n",
    "    \"uvplay_0\",\"uvplay_1\",\"uvplay_2\",\"uvplay_3\",\"uvplay_4\",\"uvplay_5\",\"uvplay_6\",\"uvplay_7\",\n",
    "    \"uvplay_8\",\"uvplay_9\",\"uvplay_10\",\"uvplay_11\",\"uvplay_12\",\"uvplay_13\",\"uvplay_14\",\"uvplay_15\",\n",
    "    \"uvplay_16\",\"uvplay_17\",\"uvplay_18\",\"uvplay_19\",\"uvplay_20\",\"uvplay_21\",\"uvplay_22\",\"uvplay_23\",\n",
    "    \"uvplay_24\",\"uvplay_25\",\"uvplay_26\",\"uvplay_27\",\"uvplay_28\",\"uvplay_29\",\"uvplay_30\",\"uvplay_31\",\"uvplay_mask\",\n",
    "    \"ufav_0\",\"ufav_1\",\"ufav_2\",\"ufav_3\",\"ufav_4\",\"ufav_5\",\"ufav_6\",\"ufav_7\",\n",
    "    \"ufav_8\",\"ufav_9\",\"ufav_10\",\"ufav_11\",\"ufav_12\",\"ufav_13\",\"ufav_14\",\"ufav_15\",\n",
    "    \"ufav_16\",\"ufav_17\",\"ufav_18\",\"ufav_19\",\"ufav_20\",\"ufav_21\",\"ufav_22\",\"ufav_23\",\n",
    "    \"ufav_24\",\"ufav_25\",\"ufav_26\",\"ufav_27\",\"ufav_28\",\"ufav_29\",\"ufav_30\",\"ufav_31\",\"ufav_mask\",\n",
    "    \"ulike_0\",\"ulike_1\",\"ulike_2\",\"ulike_3\",\"ulike_4\",\"ulike_5\",\"ulike_6\",\"ulike_7\",\n",
    "    \"ulike_8\",\"ulike_9\",\"ulike_10\",\"ulike_11\",\"ulike_12\",\"ulike_13\",\"ulike_14\",\"ulike_15\",\n",
    "    \"ulike_16\",\"ulike_17\",\"ulike_18\",\"ulike_19\",\"ulike_20\",\"ulike_21\",\"ulike_22\",\"ulike_23\",\n",
    "    \"ulike_24\",\"ulike_25\",\"ulike_26\",\"ulike_27\",\"ulike_28\",\"ulike_29\",\"ulike_30\",\"ulike_31\",\"ulike_mask\",\n",
    "    \"ushare_0\",\"ushare_1\",\"ushare_2\",\"ushare_3\",\"ushare_4\",\"ushare_5\",\"ushare_6\",\"ushare_7\",\n",
    "    \"ushare_8\",\"ushare_9\",\"ushare_10\",\"ushare_11\",\"ushare_12\",\"ushare_13\",\"ushare_14\",\"ushare_15\",\n",
    "    \"ushare_16\",\"ushare_17\",\"ushare_18\",\"ushare_19\",\"ushare_20\",\"ushare_21\",\"ushare_22\",\"ushare_23\",\n",
    "    \"ushare_24\",\"ushare_25\",\"ushare_26\",\"ushare_27\",\"ushare_28\",\"ushare_29\",\"ushare_30\",\"ushare_31\",\"ushare_mask\",\n",
    "    \"video_affinity\",\n",
    "    \"userLikeRatio1\",\"userShareRatio1\",\"userFavRatio1\",\"userCommentsRatio1\",\"userSVPRatio1\",\"userLPORatio1\",\n",
    "    \"userLikeRatio7\",\"userShareRatio7\",\"userFavRatio7\",\"userCommentsRatio7\",\"userSVPRatio7\",\"userLPORatio7\",\n",
    "    \"upcLikeRatio1D\",\"upcShareRatio1D\",\"upcFavRatio1D\",\"upcCommentRatio1D\",\"upcSVPRatio1D\",\"upcLPORatio1D\",\n",
    "    \"upcLikeRatio3D\",\"upcShareRatio3D\",\"upcFavRatio3D\",\"upcCommentRatio3D\",\"upcSVPRatio3D\",\"upcLPORatio3D\",\n",
    "    \"upcLikeRatio7D\",\"upcShareRatio7D\",\"upcFavRatio7D\",\"upcCommentRatio7D\",\"upcSVPRatio7D\",\"upcLPORatio7D\"\n",
    "]\n",
    "\n",
    "user_engaged_tags = [\n",
    "    \"engtag_0\",\"engtag_1\",\"engtag_2\",\"engtag_3\",\"engtag_4\",\n",
    "    \"engtag_5\",\"engtag_6\",\"engtag_7\",\"engtag_8\",\"engtag_9\",\n",
    "    \"engtag_10\",\"engtag_11\",\"engtag_12\",\"engtag_13\",\"engtag_14\",\n",
    "    \"engtag_15\",\"engtag_16\",\"engtag_17\",\"engtag_18\",\"engtag_19\",\n",
    "    \"engtag_20\",\"engtag_21\",\"engtag_22\",\"engtag_23\",\"engtag_24\",\n",
    "    \n",
    "    \"engtag_mask_0\",\"engtag_mask_1\",\"engtag_mask_2\",\"engtag_mask_3\",\"engtag_mask_4\",\n",
    "    \"engtag_mask_5\",\"engtag_mask_6\",\"engtag_mask_7\",\"engtag_mask_8\",\"engtag_mask_9\",\n",
    "    \"engtag_mask_10\",\"engtag_mask_11\",\"engtag_mask_12\",\"engtag_mask_13\",\"engtag_mask_14\",\n",
    "    \"engtag_mask_15\",\"engtag_mask_16\",\"engtag_mask_17\",\"engtag_mask_18\",\"engtag_mask_19\",\n",
    "    \"engtag_mask_20\",\"engtag_mask_21\",\"engtag_mask_22\",\"engtag_mask_23\",\"engtag_mask_24\"\n",
    "]\n",
    "\n",
    "post_sparse_features = [\n",
    "    'tagId'\n",
    "]\n",
    "post_dense_features = [\n",
    "    \"pvplay_0\",\"pvplay_1\",\"pvplay_2\",\"pvplay_3\",\"pvplay_4\",\"pvplay_5\",\"pvplay_6\",\"pvplay_7\",\n",
    "    \"pvplay_8\",\"pvplay_9\",\"pvplay_10\",\"pvplay_11\",\"pvplay_12\",\"pvplay_13\",\"pvplay_14\",\"pvplay_15\",\n",
    "    \"pvplay_16\",\"pvplay_17\",\"pvplay_18\",\"pvplay_19\",\"pvplay_20\",\"pvplay_21\",\"pvplay_22\",\"pvplay_23\",\n",
    "    \"pvplay_24\",\"pvplay_25\",\"pvplay_26\",\"pvplay_27\",\"pvplay_28\",\"pvplay_29\",\"pvplay_30\",\"pvplay_31\",\"pvplay_mask\",\n",
    "    \"pfav_0\",\"pfav_1\",\"pfav_2\",\"pfav_3\",\"pfav_4\",\"pfav_5\",\"pfav_6\",\"pfav_7\",\n",
    "    \"pfav_8\",\"pfav_9\",\"pfav_10\",\"pfav_11\",\"pfav_12\",\"pfav_13\",\"pfav_14\",\"pfav_15\",\n",
    "    \"pfav_16\",\"pfav_17\",\"pfav_18\",\"pfav_19\",\"pfav_20\",\"pfav_21\",\"pfav_22\",\"pfav_23\",\n",
    "    \"pfav_24\",\"pfav_25\",\"pfav_26\",\"pfav_27\",\"pfav_28\",\"pfav_29\",\"pfav_30\",\"pfav_31\",\"pfav_mask\",\n",
    "    \"plike_0\",\"plike_1\",\"plike_2\",\"plike_3\",\"plike_4\",\"plike_5\",\"plike_6\",\"plike_7\",\n",
    "    \"plike_8\",\"plike_9\",\"plike_10\",\"plike_11\",\"plike_12\",\"plike_13\",\"plike_14\",\"plike_15\",\n",
    "    \"plike_16\",\"plike_17\",\"plike_18\",\"plike_19\",\"plike_20\",\"plike_21\",\"plike_22\",\"plike_23\",\n",
    "    \"plike_24\",\"plike_25\",\"plike_26\",\"plike_27\",\"plike_28\",\"plike_29\",\"plike_30\",\"plike_31\",\"plike_mask\",\n",
    "    \"pshare_0\",\"pshare_1\",\"pshare_2\",\"pshare_3\",\"pshare_4\",\"pshare_5\",\"pshare_6\",\"pshare_7\",\n",
    "    \"pshare_8\",\"pshare_9\",\"pshare_10\",\"pshare_11\",\"pshare_12\",\"pshare_13\",\"pshare_14\",\"pshare_15\",\n",
    "    \"pshare_16\",\"pshare_17\",\"pshare_18\",\"pshare_19\",\"pshare_20\",\"pshare_21\",\"pshare_22\",\"pshare_23\",\n",
    "    \"pshare_24\",\"pshare_25\",\"pshare_26\",\"pshare_27\",\"pshare_28\",\"pshare_29\",\"pshare_30\",\"pshare_31\",\"pshare_mask\",\n",
    "    \"postLikeRatio2h\",\"postShareRatio2h\",\"postFavRatio2h\",\"postCommentRatio2h\",\"postSVPRatio2h\",\"postLPORatio2h\",\n",
    "    \"postLikeRatio1D\",\"postShareRatio1D\",\"postFavRatio1D\",\"postCommentRatio1D\",\"postSVPRatio1D\",\"postLPORatio1D\",\n",
    "    \"pcLikeRatio2h\",\"pcShareRatio2h\",\"pcFavRatio2h\",\"pcCommentRatio2h\",\"pcSVPRatio2h\",\"pcLPORatio2h\",\n",
    "    \"pcLikeRatio1D\",\"pcShareRatio1D\",\"pcFavRatio1D\",\"pcCommentRatio1D\",\"pcSVPRatio1D\",\" pcLPORatio1D\"\n",
    "]\n",
    "\n",
    "ignore_features = [\n",
    "\n",
    "]\n",
    "\n",
    "DROPOUT = 0.4\n",
    "L2REG = 1e-4\n",
    "LR = 0.001\n",
    "\n",
    "#'''\n",
    "# change this - when using the total dataset\n",
    "batch_size = 50000\n",
    "NUM_TEST_EXAMPLES =  50000     \n",
    "#NUM_TEST_EXAMPLES = 208990\n",
    "#'''\n",
    "\n",
    "'''\n",
    "# small size testing \n",
    "batch_size = 1000\n",
    "NUM_TEST_EXAMPLES = 2000\n",
    "'''\n",
    "\n",
    "\n",
    "num_of_validations = 6\n",
    "\n",
    "test_folder = \"ranker_isp_nu\"\n",
    "\n",
    "# model_folder = \"sc_ranker_debiasing_Hindi_video_wide_and_deep\"\n",
    "# model_folder = \"sc_ranker_debiasing_Hindi_video_wide_and_deep_popular_sampling\"\n",
    "# model_folder = \"sc_ranker_debiasing_Hindi_video_mask_net\"\n",
    "# model_folder = \"sc_ranker_debiasing_Hindi_video_mask_net_popular_sampling\"\n",
    "# model_folder = \"sc_ranker_debiasing_Hindi_video_mask_net_serial\"\n",
    "# model_folder = \"sc_ranker_debiasing_Hindi_video_mask_net_serial_popular_sampling\"\n",
    "# model_folder = \"sc_ranker_debiasing_Hindi_video_deep_cross_network\"\n",
    "# model_folder = \"sc_ranker_debiasing_Hindi_video_deep_cross_network_popular_sampling\"\n",
    "# model_folder = \"sc_ranker_debiasing_Hindi_video_deep_cross_network_serial\"\n",
    "model_folder = \"unified_signal_Hindi_video_mask_net_serial_sampled\"\n",
    "\n",
    "# model_name = \"wide_and_deep\"\n",
    "# model_name = \"wide_and_deep_popular\"\n",
    "# model_name = \"mask_net\"\n",
    "# model_name = \"mask_net_popular\"\n",
    "model_name = \"mask_net_serial\"\n",
    "# model_name = \"mask_net_serial_popular\"\n",
    "# model_name = \"deep_cross_net\"\n",
    "# model_name = \"deep_cross_net_popular\"\n",
    "# model_name = \"deep_cross_net_serial\"\n",
    "#model_name = \"deep_cross_net_serial_popular\"\n",
    "\n",
    "TESTDATA_DIR = \"gs://tpu-cg-us/Ashish/ranker_isp_nu\"\n",
    "\n",
    "MODEL_DIR = 'gs://sharechat-prod-bigquery-data/dca_ranker/v0/dca_ranker/2023/02/17/dca_ranker_2023_02_17_10_12_06_4821_hindi_video_model_unified/'\n",
    "MODEL_DIR_LOCAL = model_folder\n",
    "#os.system(f\"mkdir -p {MODEL_DIR_LOCAL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "012f3465-4a96-4103-83f9-c7213be56ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_distribute_input_option():\n",
    "    # Add a try...except block as OSS tensorflow_recommenders is depending on\n",
    "    # stable TF version, i.e. TF2.4.\n",
    "    try:\n",
    "        return tf.distribute.InputOptions(experimental_fetch_to_device=False)\n",
    "    except TypeError:\n",
    "        return tf.distribute.InputOptions(experimental_prefetch_to_device=False)\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class DataConfig:\n",
    "    \"\"\"Dataset config for training and evaluation.\"\"\"\n",
    "    input_path: str = ''\n",
    "    global_batch_size: int = batch_size\n",
    "    is_training: bool = True\n",
    "    dtype: str = 'float32'\n",
    "    shuffle_buffer_size: int = 10000\n",
    "    cycle_length: int = 8\n",
    "    sharding: bool = True\n",
    "    num_shards_per_host: int = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff1c1ee3-93c3-4f89-82ff-6cd37b8ec9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_mapping = pd.read_csv(\n",
    "    \"sc_ranker_debiasing-sc_ranker_debiasing_tag_index_mapping-000000000000.csv\",\n",
    "    dtype={'tagId': 'str'}\n",
    ")\n",
    "district_mapping = pd.read_csv(\n",
    "    \"userDistrict_mapping.csv\",#\"sc_ranker_debiasing-sc_ranker_debiasing_district_index_mapping-000000000000.csv\",\n",
    "    dtype={'userDistrict': 'str'}\n",
    ")\n",
    "\n",
    "tag_mapping.sort_values(by='tag_index', axis=0, inplace=True)\n",
    "tag_mapping.reset_index(drop=True, inplace=True)\n",
    "district_mapping.fillna(\"null\", inplace=True)\n",
    "\n",
    "district_mapping.sort_values(by='district_index', axis=0, inplace=True)\n",
    "district_mapping.reset_index(drop=True, inplace=True)\n",
    "district_mapping.fillna(\"null\", inplace=True)\n",
    "\n",
    "tag_index = {\n",
    "    'keys': list(tag_mapping.tagId),\n",
    "    'values': list(tag_mapping.tag_index),\n",
    "}\n",
    "\n",
    "district_index = {\n",
    "    'keys': list(district_mapping.userDistrict),\n",
    "    'values': list(district_mapping.district_index),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "150c61de-b32e-470e-8e3d-4dd56c3628bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVReader(object):\n",
    "    def __init__(self, params, num_labels, field_delim=\",\", use_fake_data=False):\n",
    "        self.params = params\n",
    "        self.num_labels = num_labels\n",
    "        self.field_delim = field_delim\n",
    "        self._use_fake_data = use_fake_data\n",
    "        \n",
    "        self.tag_index = tf.lookup.StaticHashTable(\n",
    "            tf.lookup.KeyValueTensorInitializer(tag_index['keys'], tag_index['values']),\n",
    "            default_value=0\n",
    "        )\n",
    "        \n",
    "        self.district_index = tf.lookup.StaticHashTable(\n",
    "            tf.lookup.KeyValueTensorInitializer(district_index['keys'], district_index['values']),\n",
    "            default_value=0\n",
    "        )\n",
    "        \n",
    "    def __call__(self, ctx: tf.distribute.InputContext):\n",
    "        params = self.params\n",
    "        batch_size = ctx.get_per_replica_batch_size(\n",
    "            params.global_batch_size\n",
    "        ) if ctx else params.global_batch_size\n",
    "        \n",
    "        @tf.function\n",
    "        def _parse_fn(example):\n",
    "            num_sparse_features = len(vocab_sizes)\n",
    "            \n",
    "          \n",
    "            meta_defaults = [''] * len(meta)\n",
    "            \n",
    "            \n",
    "            label_defaults = [0.0] * num_labels\n",
    "            \n",
    "            other_feat_defaults = [0.0] * (hour_feat+dayofweek+num_other_features)\n",
    "            \n",
    "            post_sparse_defaults = ['0'] * len(post_sparse_features)\n",
    "            post_dense_defaults = [-1.0] * len(post_dense_features)\n",
    "            \n",
    "            user_sparse_defaults = ['0'] * len(user_sparse_features)\n",
    "            user_dense_defaults = [-1.0] * len(user_dense_features)\n",
    "            user_engaged_tags_defaults = ['0'] * (len(user_engaged_tags)//2) + [0.0] * (len(user_engaged_tags)//2)\n",
    "\n",
    "            record_defaults =   meta_defaults + label_defaults + \\\n",
    "                                other_feat_defaults + \\\n",
    "                                post_sparse_defaults + \\\n",
    "                                post_dense_defaults + \\\n",
    "                                user_sparse_defaults + \\\n",
    "                                user_dense_defaults + \\\n",
    "                                user_engaged_tags_defaults\n",
    "\n",
    "            fields = tf.io.decode_csv(example, record_defaults,\n",
    "                                      field_delim=self.field_delim, na_value='')\n",
    "            \n",
    "            offset = 0\n",
    "            \n",
    "            \n",
    "            meta_feats = {}\n",
    "            for idx in range(len(meta)):\n",
    "                if col_names[idx+offset] in ignore_features:\n",
    "                    continue\n",
    "                meta_feats[col_names[idx+offset]] = fields[idx+offset]\n",
    "            #meta_feats[\"post_popularity\"] = fields[59]\n",
    "            meta_feats[\"post_popularity\"] = fields[163]\n",
    "            offset += len(meta)\n",
    "            \n",
    "            \n",
    "            #label = fields[offset+0]\n",
    "            label = tf.cast(fields[offset+0], tf.float32)\n",
    "            offset += num_labels\n",
    "\n",
    "            features = {'time': {}, 'sparse_features': {} , 'meta': meta_feats} \n",
    "\n",
    "            for idx in range(hour_feat+dayofweek+num_other_features):\n",
    "                if col_names[idx+offset] in ignore_features:\n",
    "                    continue\n",
    "                if col_names[idx+offset] in ('hour', 'dayofweek'):\n",
    "                    features['time'][col_names[idx+offset]] = tf.cast(fields[idx+offset], tf.int32)\n",
    "                else:\n",
    "                    features['time'][col_names[idx+offset]] = tf.cast(tf.expand_dims(fields[idx+offset], axis=-1), tf.float32)\n",
    "            offset += hour_feat+dayofweek+num_other_features\n",
    "\n",
    "            \n",
    "            for idx in range(len(post_sparse_features)):\n",
    "                if col_names[idx+offset] in ignore_features:\n",
    "                    continue\n",
    "                    \n",
    "                if col_names[idx+offset] == \"tagId\":\n",
    "                    features['sparse_features'][col_names[idx+offset]] = self.tag_index.lookup(fields[idx+offset])\n",
    "                else:\n",
    "                    features['sparse_features'][col_names[idx+offset]] = fields[idx+offset]\n",
    "            offset += len(post_sparse_features)\n",
    "\n",
    "            feat = []\n",
    "            post_embed = []\n",
    "            for idx in range(len(post_dense_features)):\n",
    "                if col_names[idx+offset] in ignore_features:\n",
    "                    continue\n",
    "                if col_names[idx+offset].startswith('pvplay') or col_names[idx+offset].startswith('pfav') or col_names[idx+offset].startswith('plike') or col_names[idx+offset].startswith('pshare'):\n",
    "                    post_embed.append(fields[idx + offset])\n",
    "                feat.append(fields[idx + offset])\n",
    "            features['post_dense_features'] = tf.stack(feat, axis=1)\n",
    "            features['post_embed'] = tf.stack(post_embed, axis=1)\n",
    "            offset += len(post_dense_features)\n",
    "            \n",
    "            \n",
    "            for idx in range(len(user_sparse_features)):\n",
    "                if col_names[idx+offset] in ignore_features:\n",
    "                    continue\n",
    "                if col_names[idx+offset] == \"userDistrict\":\n",
    "                    features['sparse_features'][col_names[idx+offset]] = self.district_index.lookup(fields[idx+offset])\n",
    "                else:\n",
    "                    features['sparse_features'][col_names[idx+offset]] = fields[idx+offset]\n",
    "            offset += len(user_sparse_features)\n",
    "            \n",
    "            feat = []\n",
    "            user_embed = []\n",
    "            for idx in range(len(user_dense_features)):\n",
    "                if col_names[idx+offset] in ignore_features:\n",
    "                    continue\n",
    "                if col_names[idx+offset].startswith('uvplay') or col_names[idx+offset].startswith('ufav') or col_names[idx+offset].startswith('ulike') or col_names[idx+offset].startswith('ushare'):\n",
    "                    user_embed.append(fields[idx + offset])\n",
    "                feat.append(fields[idx + offset])\n",
    "            features['user_dense_features'] = tf.stack(feat, axis=1)\n",
    "            features['user_embed'] = tf.stack(user_embed, axis=1)\n",
    "            offset += len(user_dense_features)\n",
    "            \n",
    "            \n",
    "            eng_tags_mask = []\n",
    "            eng_tags = []\n",
    "            for idx in range(len(user_engaged_tags)):\n",
    "                if col_names[idx+offset] in ignore_features:\n",
    "                    continue\n",
    "                if 'mask' in col_names[idx+offset]:\n",
    "                    eng_tags_mask.append(fields[idx + offset])\n",
    "                else:\n",
    "                    eng_tags.append(self.tag_index.lookup(fields[idx + offset]))\n",
    "            features['sparse_features']['eng_tags'] = tf.stack(eng_tags, axis=1)\n",
    "            features['eng_tags_mask'] = tf.stack(eng_tags_mask, axis=1)\n",
    "            offset += len(user_engaged_tags)\n",
    "            \n",
    "            return features, label\n",
    "        \n",
    "        filenames = tf.data.Dataset.list_files(params.input_path, shuffle=False)\n",
    "        \n",
    "        if params.sharding and ctx and ctx.num_input_pipelines > 1:\n",
    "            filenames = filenames.shard(ctx.num_input_pipelines, ctx.input_pipeline_id)\n",
    "            \n",
    "        num_shards_per_host = 1\n",
    "        if params.sharding:\n",
    "            num_shards_per_host = params.num_shards_per_host\n",
    "\n",
    "        def make_dataset(shard_index):\n",
    "            filenames_for_shard = filenames.shard(num_shards_per_host, shard_index)\n",
    "            dataset = tf.data.TextLineDataset(filenames_for_shard)\n",
    "            dataset = dataset.batch(batch_size, drop_remainder=False)\n",
    "            dataset = dataset.map(_parse_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "            \n",
    "            \n",
    "            return dataset\n",
    "        indices = tf.data.Dataset.range(num_shards_per_host)\n",
    "        dataset = indices.interleave(\n",
    "            map_func=make_dataset,\n",
    "            cycle_length=params.cycle_length,\n",
    "            num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "        )\n",
    "        dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        \n",
    "        if self._use_fake_data:\n",
    "            dataset = dataset.take(1).cache().repeat()\n",
    "            \n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0820984a-6ec6-46c2-bef8-758b01e99f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://tpu-cg-us/Ashish/ranker_isp_nu'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TESTDATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "848ef4c5-d539-46db-8568-5969e92af72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-28 04:27:57.504597: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-02-28 04:27:57.504658: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-28 04:27:57.504685: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ashish-cpu): /proc/driver/nvidia/version does not exist\n",
      "2023-02-28 04:27:57.505168: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_steps: 1\n"
     ]
    }
   ],
   "source": [
    "test_params = DataConfig(\n",
    "    input_path=f'{TESTDATA_DIR}/*',\n",
    "    is_training=False,\n",
    "    sharding=False\n",
    ")\n",
    "test_dataset_callable = CSVReader(\n",
    "    params=test_params,\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "test_dataset = strategy.distribute_datasets_from_function(\n",
    "    dataset_fn=test_dataset_callable,\n",
    "    options=create_distribute_input_option()\n",
    ")\n",
    "\n",
    "test_steps = NUM_TEST_EXAMPLES // batch_size\n",
    "\n",
    "print(f\"test_steps: {test_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a9aa641-bde4-462c-b7f1-8067b569e686",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskNetModelSerial(tfrs.models.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rescale_factor = 2.0\n",
    "        \n",
    "        self.tag_embedding = tf.keras.layers.Embedding(\n",
    "                input_dim=vocab_sizes['tagId'],\n",
    "                output_dim=embedding_dims['tagId'],\n",
    "                input_length=1\n",
    "        )\n",
    "        self.eng_tag_embedding = tf.keras.layers.Embedding(\n",
    "                input_dim=vocab_sizes['tagId'],\n",
    "                output_dim=embedding_dims['tagId'],\n",
    "                input_length=25\n",
    "        )\n",
    "\n",
    "        self.district_embedding = tf.keras.layers.Embedding(\n",
    "                input_dim=vocab_sizes['userDistrict'],\n",
    "                output_dim=embedding_dims['userDistrict'],\n",
    "                input_length=1\n",
    "        )\n",
    "        \n",
    "        with tf.compat.v1.variable_scope(\"MaskBlock_time\"):\n",
    "            self.time_mask = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(\n",
    "                  units=int(feature_shapes['time']*self.rescale_factor),\n",
    "                  kernel_initializer=tf.keras.initializers.VarianceScaling(),\n",
    "                  kernel_regularizer=tf.keras.regularizers.L2(L2REG),\n",
    "                  activation=\"relu\"\n",
    "                ),\n",
    "                tf.keras.layers.Dense(\n",
    "                  units=feature_shapes['time'],\n",
    "                  kernel_initializer=tf.keras.initializers.VarianceScaling(),\n",
    "                  kernel_regularizer=tf.keras.regularizers.L2(L2REG),\n",
    "                )\n",
    "            ], name=\"InstanceGuidedMask_time\")\n",
    "            self.time_norm = tf.keras.layers.LayerNormalization()\n",
    "            self.time_mask_emb = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(\n",
    "                  units=feature_shapes['user_embed'] + feature_shapes['userDistrict'],\n",
    "                  kernel_initializer=tf.keras.initializers.VarianceScaling(),\n",
    "                  kernel_regularizer=tf.keras.regularizers.L2(L2REG),\n",
    "                ),\n",
    "                tf.keras.layers.LayerNormalization()\n",
    "            ], name=\"MaskBlock_time\")\n",
    "            \n",
    "        with tf.compat.v1.variable_scope(\"MaskBlock_user_sparse\"):\n",
    "            self.user_sparse_mask = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(\n",
    "                  units=int((feature_shapes['user_embed'] + feature_shapes['userDistrict'])\n",
    "                            *self.rescale_factor),\n",
    "                  kernel_initializer=tf.keras.initializers.VarianceScaling(),\n",
    "                  kernel_regularizer=tf.keras.regularizers.L2(L2REG),\n",
    "                  activation=\"relu\"\n",
    "                ),\n",
    "                tf.keras.layers.Dense(\n",
    "                  units=(feature_shapes['user_embed'] + feature_shapes['userDistrict']),\n",
    "                  kernel_initializer=tf.keras.initializers.VarianceScaling(),\n",
    "                  kernel_regularizer=tf.keras.regularizers.L2(L2REG),\n",
    "                )\n",
    "            ], name=\"InstanceGuidedMask_user_sparse\")\n",
    "            self.user_sparse_mask_emb = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(\n",
    "                  units=feature_shapes['eng_tags'],\n",
    "                  kernel_initializer=tf.keras.initializers.VarianceScaling(),\n",
    "                  kernel_regularizer=tf.keras.regularizers.L2(L2REG),\n",
    "                ),\n",
    "                tf.keras.layers.LayerNormalization()\n",
    "            ], name=\"MaskBlock_user_sparse\")\n",
    "            \n",
    "        with tf.compat.v1.variable_scope(\"MaskBlock_user_tags\"):\n",
    "            self.user_tags_mask = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(\n",
    "                  units=int(feature_shapes['eng_tags']*self.rescale_factor),\n",
    "                  kernel_initializer=tf.keras.initializers.VarianceScaling(),\n",
    "                  kernel_regularizer=tf.keras.regularizers.L2(L2REG),\n",
    "                  activation=\"relu\"\n",
    "                ),\n",
    "                tf.keras.layers.Dense(\n",
    "                  units=feature_shapes['eng_tags'],\n",
    "                  kernel_initializer=tf.keras.initializers.VarianceScaling(),\n",
    "                  kernel_regularizer=tf.keras.regularizers.L2(L2REG),\n",
    "                )\n",
    "            ], name=\"InstanceGuidedMask_user_tags\")\n",
    "            self.user_tags_mask_emb = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(\n",
    "                  units=feature_shapes['user_dense_features'],\n",
    "                  kernel_initializer=tf.keras.initializers.VarianceScaling(),\n",
    "                  kernel_regularizer=tf.keras.regularizers.L2(L2REG),\n",
    "                ),\n",
    "                tf.keras.layers.LayerNormalization()\n",
    "            ], name=\"MaskBlock_user_tags\")\n",
    "        \n",
    "        with tf.compat.v1.variable_scope(\"MaskBlock_user_dense\"):\n",
    "            self.user_dense_mask = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(\n",
    "                  units=int(feature_shapes['user_dense_features']*self.rescale_factor),\n",
    "                  kernel_initializer=tf.keras.initializers.VarianceScaling(),\n",
    "                  kernel_regularizer=tf.keras.regularizers.L2(L2REG),\n",
    "                  activation=\"relu\"\n",
    "                ),\n",
    "                tf.keras.layers.Dense(\n",
    "                  units=feature_shapes['user_dense_features'],\n",
    "                  kernel_initializer=tf.keras.initializers.VarianceScaling(),\n",
    "                  kernel_regularizer=tf.keras.regularizers.L2(L2REG),\n",
    "                )\n",
    "            ], name=\"InstanceGuidedMask_user_dense\")\n",
    "            self.user_dense_mask_emb = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(\n",
    "                  units=(feature_shapes['post_embed'] + feature_shapes['tagId']),\n",
    "                  kernel_initializer=tf.keras.initializers.VarianceScaling(),\n",
    "                  kernel_regularizer=tf.keras.regularizers.L2(L2REG),\n",
    "                ),\n",
    "                tf.keras.layers.LayerNormalization()\n",
    "            ], name=\"MaskBlock_user_dense\")\n",
    "            \n",
    "        with tf.compat.v1.variable_scope(\"MaskBlock_post_sparse\"):\n",
    "            self.post_sparse_mask = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(\n",
    "                  units=int((feature_shapes['post_embed'] + feature_shapes['tagId'])\n",
    "                            *self.rescale_factor),\n",
    "                  kernel_initializer=tf.keras.initializers.VarianceScaling(),\n",
    "                  kernel_regularizer=tf.keras.regularizers.L2(L2REG),\n",
    "                  activation=\"relu\"\n",
    "                ),\n",
    "                tf.keras.layers.Dense(\n",
    "                  units=(feature_shapes['post_embed'] + feature_shapes['tagId']),\n",
    "                  kernel_initializer=tf.keras.initializers.VarianceScaling(),\n",
    "                  kernel_regularizer=tf.keras.regularizers.L2(L2REG),\n",
    "                )\n",
    "            ], name=\"InstanceGuidedMask_post_sparse\")\n",
    "            self.post_sparse_mask_emb = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(\n",
    "                  units=feature_shapes['post_dense_features'],\n",
    "                  kernel_initializer=tf.keras.initializers.VarianceScaling(),\n",
    "                  kernel_regularizer=tf.keras.regularizers.L2(L2REG),\n",
    "                ),\n",
    "                tf.keras.layers.LayerNormalization()\n",
    "            ], name=\"MaskBlock_post_sparse\")\n",
    "        \n",
    "        with tf.compat.v1.variable_scope(\"MaskBlock_post_dense\"):\n",
    "            self.post_dense_mask = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(\n",
    "                  units=int(feature_shapes['post_dense_features']*self.rescale_factor),\n",
    "                  kernel_initializer=tf.keras.initializers.VarianceScaling(),\n",
    "                  kernel_regularizer=tf.keras.regularizers.L2(L2REG),\n",
    "                  activation=\"relu\"\n",
    "                ),\n",
    "                tf.keras.layers.Dense(\n",
    "                  units=feature_shapes['post_dense_features'],\n",
    "                  kernel_initializer=tf.keras.initializers.VarianceScaling(),\n",
    "                  kernel_regularizer=tf.keras.regularizers.L2(L2REG),\n",
    "                )\n",
    "            ], name=\"InstanceGuidedMask_post_dense\")\n",
    "            self.post_dense_mask_emb = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(\n",
    "                  units=feature_shapes['post_dense_features'],\n",
    "                  kernel_initializer=tf.keras.initializers.VarianceScaling(),\n",
    "                  kernel_regularizer=tf.keras.regularizers.L2(L2REG),\n",
    "                ),\n",
    "                tf.keras.layers.LayerNormalization()\n",
    "            ], name=\"MaskBlock_post_dense\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        with tf.compat.v1.variable_scope(\"ClassificationTower\"):\n",
    "            self.classification_tower = tf.keras.Sequential([\n",
    "              tf.keras.layers.Dense(\n",
    "                  units=1,\n",
    "                  kernel_initializer=tf.keras.initializers.VarianceScaling(),\n",
    "                  kernel_regularizer=tf.keras.regularizers.L2(L2REG),\n",
    "              )\n",
    "            ])\n",
    "\n",
    "        self.final_activation = tf.keras.layers.Activation('relu')\n",
    "        \n",
    "        self.task: tf.keras.layers.Layer = tfrs.tasks.Ranking(\n",
    "            #loss=tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE),\n",
    "            loss=tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE),\n",
    "            metrics=[\n",
    "                    # tf.keras.metrics.AUC(name=\"auc\"),\n",
    "                    # tf.keras.metrics.AUC(curve=\"PR\", name=\"pr-auc\"),\n",
    "                    # tf.keras.metrics.Precision(name=\"precision\"),\n",
    "                    # tf.keras.metrics.Recall(name=\"recall\"),\n",
    "                    # tf.keras.metrics.TruePositives(name=\"TP\"),\n",
    "                    # tf.keras.metrics.FalsePositives(name=\"FP\"),\n",
    "                    # tf.keras.metrics.CategoricalAccuracy(name='categorical_accuracy'),\n",
    "                    tf.keras.metrics.Accuracy(name='accuracy'),\n",
    "                    tf.keras.metrics.CosineSimilarity(name='cosine_similarity', axis=-1),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        \n",
    "    def compute_loss(self, inputs, training=False) -> tf.Tensor:\n",
    "        if len(inputs) == 2:\n",
    "            features, labels = inputs\n",
    "            rating_predictions = self(features)\n",
    "            loss = self.task(labels=labels, predictions=rating_predictions)\n",
    "        elif len(inputs) == 3:\n",
    "            features, labels, sample_weight = inputs\n",
    "            rating_predictions = self(features)\n",
    "            loss = self.task(labels=labels, predictions=rating_predictions, sample_weight=sample_weight)\n",
    "        \n",
    "        loss = tf.reduce_mean(loss)\n",
    "        return loss / tf.distribute.get_strategy().num_replicas_in_sync\n",
    "    \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        sparse_features = inputs[\"sparse_features\"]\n",
    "\n",
    "        tag_embed = self.tag_embedding(sparse_features['tagId'])\n",
    "        eng_tag_embed = self.tag_embedding(sparse_features['eng_tags'])\n",
    "        sequence_length = tf.math.reduce_sum(inputs['eng_tags_mask'], axis=1, keepdims=True) + 0.0001\n",
    "        \n",
    "        eng_tag_embed = tf.math.divide(tf.math.reduce_sum(eng_tag_embed, axis=1), sequence_length)\n",
    "        district_embed = self.district_embedding(sparse_features['userDistrict'])\n",
    "        \n",
    "        hour = tf.one_hot(inputs['time']['hour'], 24)\n",
    "        dayofweek = tf.one_hot(inputs['time']['dayofweek'], 7)\n",
    "        time = tf.keras.layers.Concatenate(axis=-1)([\n",
    "            hour, dayofweek,\n",
    "            inputs['time']['is_weekend'],\n",
    "            inputs['time']['is_morning'],\n",
    "            inputs['time']['is_afternoon'],\n",
    "            inputs['time']['is_evening'],\n",
    "            inputs['time']['is_night'],\n",
    "        ])\n",
    "        user_sparse = tf.keras.layers.Concatenate()([inputs['user_embed'], district_embed])\n",
    "        post_sparse = tf.keras.layers.Concatenate()([inputs['post_embed'], tag_embed])\n",
    "        user_dense = inputs['user_dense_features']\n",
    "        post_dense = inputs['post_dense_features']\n",
    "\n",
    "        \n",
    "        time_norm = self.time_norm(time)\n",
    "        time_mask = self.time_mask(time)\n",
    "        time_mask_emb = self.time_mask_emb(tf.keras.layers.Multiply()([time_norm, time_mask]))\n",
    "        \n",
    "        \n",
    "        #user_sparse_norm = self.user_sparse_norm(user_sparse)\n",
    "        user_sparse_mask = self.user_sparse_mask(user_sparse)\n",
    "        user_sparse_mask_emb = self.user_sparse_mask_emb(tf.keras.layers.Multiply()([time_mask_emb, user_sparse_mask]))\n",
    "        \n",
    "#         user_tags_norm = self.user_tags_norm(eng_tag_embed)\n",
    "        user_tags_mask = self.user_tags_mask(eng_tag_embed)\n",
    "        user_tags_mask_emb = self.user_tags_mask_emb(tf.keras.layers.Multiply()([user_sparse_mask_emb, user_tags_mask]))\n",
    "        \n",
    "#         user_dense_norm = self.user_dense_norm(user_dense)\n",
    "        user_dense_mask = self.user_dense_mask(user_dense)\n",
    "        user_dense_mask_emb = self.user_dense_mask_emb(tf.keras.layers.Multiply()([user_tags_mask_emb, user_dense_mask]))\n",
    "        \n",
    "        \n",
    "#         post_sparse_norm = self.post_sparse_norm(post_sparse)\n",
    "        post_sparse_mask = self.post_sparse_mask(post_sparse)\n",
    "        post_sparse_mask_emb = self.post_sparse_mask_emb(tf.keras.layers.Multiply()([user_dense_mask_emb, post_sparse_mask]))\n",
    "        \n",
    "#         post_dense_norm = self.post_dense_norm(post_dense)\n",
    "        post_dense_mask = self.post_dense_mask(post_dense)\n",
    "        post_dense_mask_emb = self.post_dense_mask_emb(tf.keras.layers.Multiply()([post_sparse_mask_emb, post_dense_mask]))\n",
    "        \n",
    "        \n",
    "        vector = post_dense_mask_emb\n",
    "        \n",
    "        logits = self.classification_tower(vector)\n",
    "        \n",
    "        prediction = self.final_activation(logits)\n",
    "        \n",
    "        return tf.reshape(prediction, [-1])\n",
    "\n",
    "    @property\n",
    "    def embedding_trainable_variables(self) -> Sequence[tf.Variable]:\n",
    "        return [] #self.embedding_layer.trainable_variables\n",
    "\n",
    "    @property\n",
    "    def deep_trainable_variables(self) -> Sequence[tf.Variable]:\n",
    "        dense_vars = []\n",
    "        for layer in self.layers:\n",
    "#             if layer != self.embedding_layer:\n",
    "            dense_vars.extend(layer.trainable_variables)\n",
    "        return dense_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c29e0206-d3df-4cea-a691-ede95f3ffdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_shapes = {\n",
    "    'time': 36,\n",
    "    'post_dense_features': 156,\n",
    "    'user_dense_features': 163,\n",
    "    'user_embed': 132,\n",
    "    'post_embed': 132,\n",
    "    'tagId': embedding_dims['tagId'],\n",
    "    'userDistrict': embedding_dims['userDistrict'],\n",
    "    'eng_tags': embedding_dims['tagId']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa100878-fc6e-45f6-b8ec-0069ec694b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    # embedding_optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
    "    # deep_optimizer = tf.keras.optimizers.Adagrad(lr=0.1)\n",
    "\n",
    "    \n",
    "    model = MaskNetModelSerial()\n",
    "    \n",
    "\n",
    "    # optimizer = tfrs.experimental.optimizers.CompositeOptimizer([\n",
    "    #     (embedding_optimizer, lambda: model.embedding_trainable_variables),\n",
    "    #     (deep_optimizer, lambda: model.deep_trainable_variables),\n",
    "    # ])\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=0.0005)\n",
    "    \n",
    "    model.load_weights(MODEL_DIR + 'export/variables/variables')\n",
    "    \n",
    "    model.compile(optimizer)\n",
    "    #model.load_weights('gs://deep-ctr/devansh_production/checkpoints/my_checkpoint2')\n",
    "    #model = tf.keras.models.load_model('gs://deep-ctr/devansh_production/models2/')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac01efa8-74e2-4743-8fab-c6d2ba82ff8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [01:30, 30.04s/it]                                                                                                                         \n"
     ]
    }
   ],
   "source": [
    "pred_df = {\n",
    "    'isp_date': [],\n",
    "    'postId': [],\n",
    "    'userId':[],\n",
    "    'model': [],\n",
    "    'isp_score': [],\n",
    "    'groundtruth': [],\n",
    "    'prediction': [],\n",
    "    'production_model_prediction_unified': [],\n",
    "    'production_model_prediction_combined': [],\n",
    "}\n",
    "\n",
    "prediction = []\n",
    "groundtruth = []\n",
    "production_model_prediction = []\n",
    "\n",
    "progress_bar = tqdm(total=test_steps + 1)\n",
    "\n",
    "\n",
    "for ind, example in enumerate(test_dataset):\n",
    "    pred = model(example[0])\n",
    "    pred_df['isp_date'] += list(example[0]['meta']['isp_date'].numpy().astype('str'))\n",
    "    pred_df['model'] += list(example[0]['meta']['model'].numpy().astype('str'))\n",
    "    pred_df['postId'] += list(example[0]['meta']['postId'].numpy().astype('str'))\n",
    "    pred_df['userId'] += list(example[0]['meta']['userId'].numpy().astype('str'))\n",
    "#     pred_df['user_embed'] += list(example[0]['user_embed'].numpy().astype('str'))\n",
    "#     pred_df['post_embed'] += list(example[0]['post_embed'].numpy().astype('str'))\n",
    "    pred_df['isp_score'] += list(example[0]['meta']['isp_score'].numpy().astype('str'))\n",
    "#     print(\"pred_df['isp_date'] \",pred_df['isp_date'])\n",
    "#     pred_df['L1'] += list(example[0]['meta']['L1'].numpy().astype('str'))\n",
    "#     pred_df['L2'] += list(example[0]['meta']['L2'].numpy().astype('str'))\n",
    "    pred_df['production_model_prediction_combined'] += list(example[0]['meta']['combined_score'].numpy().astype('float32'))\n",
    "\n",
    "#     pred_df['groundtruth'] += list(example[1].numpy())\n",
    "    pred_df['groundtruth'] += list(example[0]['meta']['video_play'].numpy().astype('float32'))\n",
    "    pred_df['prediction'] += list(pred.numpy())\n",
    "    \n",
    "    pred_df['production_model_prediction_unified'] += list(example[0]['meta']['unified_signal1'].numpy().astype('float32'))\n",
    "    \n",
    "    progress_bar.update(1)\n",
    "    if ind > test_steps:\n",
    "        break\n",
    "    \n",
    "\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e43f4336-d74e-436f-af4f-6ca21f617e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DCG(labels, perm, top=10):\n",
    "    result = 0.0\n",
    "    for i in range(min(top, len(perm))):\n",
    "        result += labels[perm[i]] / np.log(i+2)\n",
    "    return result\n",
    "\n",
    "def NDCG(labels, preds, top=10):\n",
    "    args = np.argsort(-preds)\n",
    "    iargs = np.argsort(-labels)\n",
    "    iDCG = DCG(labels, iargs, top=top)\n",
    "    if iDCG < 1e-3:\n",
    "        return 0.0, 0\n",
    "    return DCG(labels, args, top=top) / iDCG, 1\n",
    "\n",
    "def calc_NDCG(df, preds_field, label_field=\"groundtruth\", top=10):\n",
    "    dfg = df.groupby(\"userId\")\n",
    "    result = 0\n",
    "    count = 0\n",
    "    for _, group in dfg:\n",
    "        if len(group) <= 1:\n",
    "            continue\n",
    "        labels = group[label_field].values\n",
    "        preds = group[preds_field].values\n",
    "        result_a, count_a = NDCG(labels, preds, top=top)\n",
    "        result += result_a\n",
    "        count += count_a\n",
    "    result /= count\n",
    "    return result\n",
    "\n",
    "def naive_roc_auc_score(y_true, y_pred):\n",
    "    num_same_sign = 0\n",
    "    num_pairs = 0\n",
    "    \n",
    "    for a in range(len(y_true)):\n",
    "        for b in range(len(y_true)):\n",
    "            if y_true[a] > y_true[b]:\n",
    "                num_pairs += 1\n",
    "            if y_pred[a] > y_pred[b]:\n",
    "                num_same_sign += 1\n",
    "            elif y_pred[a] == y_pred[b]:\n",
    "                num_same_sign += .5\n",
    "                \n",
    "    return num_same_sign / num_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fd8039d-0c2c-4b09-b51a-47b3947113aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_g = pd.DataFrame.from_dict(pred_df).groupby(['userId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01a22ae5-a287-4a7d-bbd1-69c4ad901a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_data = pd.DataFrame.from_dict(pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e853585-61e4-4c05-848f-cc3839656d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_data = pd_data.sort_values('userId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04ae1a6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4740515"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_data['production_model_prediction_combined'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47100638",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_data['prediction2'] = 1 / (1 + np.exp(-pd_data.prediction))\n",
    "pd_data['production_model_prediction2'] = 1 / (1 + np.exp(-pd_data.production_model_prediction_combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c007f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_data2 = pd_data[(~pd_data['isp_score'].isnull()) & (pd_data['isp_score']!='')]\n",
    "\n",
    "# naive_roc_auc_score(pd_data['production_model_prediction_combined'],pd_data['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dde43287-bc23-42fc-91d6-96da128d6908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48693290127526545"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_NDCG(pd_data, \"prediction2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1f55c67-a4ed-4204-b4af-61eefa131f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9881099539665799"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_NDCG(pd_data, \"production_model_prediction_unified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "395c69ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9963664246817104"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_NDCG(pd_data, \"production_model_prediction_combined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "336da741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isp_date</th>\n",
       "      <th>postId</th>\n",
       "      <th>userId</th>\n",
       "      <th>model</th>\n",
       "      <th>isp_score</th>\n",
       "      <th>groundtruth</th>\n",
       "      <th>prediction</th>\n",
       "      <th>production_model_prediction_unified</th>\n",
       "      <th>production_model_prediction_combined</th>\n",
       "      <th>prediction2</th>\n",
       "      <th>production_model_prediction2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44918</th>\n",
       "      <td></td>\n",
       "      <td>5371007635</td>\n",
       "      <td>1000230543</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.196758</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.549031</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72087</th>\n",
       "      <td></td>\n",
       "      <td>7453168735</td>\n",
       "      <td>1000407105</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.530007</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.629485</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72086</th>\n",
       "      <td>2023-02-22 10:35:35 UTC</td>\n",
       "      <td>1407124735</td>\n",
       "      <td>1000407105</td>\n",
       "      <td>diversity_sugg_video_30d_inc_15s_vid</td>\n",
       "      <td>1.773505</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.398322</td>\n",
       "      <td>1.6595</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.598284</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72085</th>\n",
       "      <td></td>\n",
       "      <td>9712862835</td>\n",
       "      <td>1000407105</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.152350</td>\n",
       "      <td>1.6200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538014</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72084</th>\n",
       "      <td>2023-02-22 10:36:24 UTC</td>\n",
       "      <td>1407124735</td>\n",
       "      <td>1000407105</td>\n",
       "      <td>diversity_sugg_video_30d_inc_15s_vid</td>\n",
       "      <td>2.2167487</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.398322</td>\n",
       "      <td>1.6595</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.598284</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      isp_date      postId      userId  \\\n",
       "44918                           5371007635  1000230543   \n",
       "72087                           7453168735  1000407105   \n",
       "72086  2023-02-22 10:35:35 UTC  1407124735  1000407105   \n",
       "72085                           9712862835  1000407105   \n",
       "72084  2023-02-22 10:36:24 UTC  1407124735  1000407105   \n",
       "\n",
       "                                      model  isp_score  groundtruth  \\\n",
       "44918                                                           0.0   \n",
       "72087                                                           0.0   \n",
       "72086  diversity_sugg_video_30d_inc_15s_vid   1.773505          0.0   \n",
       "72085                                                           0.0   \n",
       "72084  diversity_sugg_video_30d_inc_15s_vid  2.2167487          0.0   \n",
       "\n",
       "       prediction  production_model_prediction_unified  \\\n",
       "44918    0.196758                               0.0000   \n",
       "72087    0.530007                               0.0000   \n",
       "72086    0.398322                               1.6595   \n",
       "72085    0.152350                               1.6200   \n",
       "72084    0.398322                               1.6595   \n",
       "\n",
       "       production_model_prediction_combined  prediction2  \\\n",
       "44918                                   0.0     0.549031   \n",
       "72087                                   0.0     0.629485   \n",
       "72086                                   0.0     0.598284   \n",
       "72085                                   0.0     0.538014   \n",
       "72084                                   0.0     0.598284   \n",
       "\n",
       "       production_model_prediction2  \n",
       "44918                           0.5  \n",
       "72087                           0.5  \n",
       "72086                           0.5  \n",
       "72085                           0.5  \n",
       "72084                           0.5  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a70a05",
   "metadata": {},
   "source": [
    "## Production AUC based on video_play as ground truth signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a656461d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6521219651574703"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr, tpr, thresholds = roc_curve(pd_data2['groundtruth'], pd_data2['prediction2'])\n",
    "auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7dd996",
   "metadata": {},
   "source": [
    "## Production RMSE based on video_play as ground truth signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e1db18f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "production rmse_score  0.5621443524854045\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "rmse_score = math.sqrt(mean_squared_error(pd_data2['groundtruth'], pd_data2['prediction2']))\n",
    "print(\"production rmse_score \",rmse_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da6517c",
   "metadata": {},
   "source": [
    "## Percentage of rows having ISP score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2438b615-27da-4374-b7ed-1dda2a538d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.703333333333334"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(pd_data2)/len(pd_data))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1efb0baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pandas/core/generic.py:5516: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "pd_data2.isp_score = pd_data2.isp_score.astype('float32')\n",
    "pd_data2['isp_score2'] = 1 / (1 + np.exp(-pd_data2.isp_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "345bef6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd_data2[['isp_score2','prediction2']]\n",
    "y = pd_data2['groundtruth']\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "lr.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "eb35d4fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.19899318,  5.83047545]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1b57044d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse_score  0.43164618432214585\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "tot_score = np.add([-0.04178833*j for j in pd_data2['isp_score2'].tolist()],[0.5406057*_ for _ in pd_data2['prediction2'].tolist()])\n",
    "rmse_score = math.sqrt(mean_squared_error(y, tot_score))#dfk['pred']))#df['pred']))#tot_score0.7257371784077328\n",
    "print(\"rmse_score \",rmse_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d5fc0b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " auc is  0.6529187432643824\n"
     ]
    }
   ],
   "source": [
    "tot_score = np.add([-0.04178833*j for j in pd_data2['isp_score2'].tolist()],[0.5406057*_ for _ in pd_data2['prediction2'].tolist()])\n",
    "fpr, tpr, thresholds = roc_curve(pd_data2['groundtruth'], tot_score, pos_label=1)\n",
    "print(\" auc is \",auc(fpr, tpr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dd942c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " auc is  0.6528519618884225\n"
     ]
    }
   ],
   "source": [
    "tot_score = np.add([-0.19899318*j for j in pd_data2['isp_score2'].tolist()],[5.83047545*_ for _ in pd_data2['prediction2'].tolist()])\n",
    "fpr, tpr, thresholds = roc_curve(pd_data2['groundtruth'], tot_score, pos_label=1)\n",
    "print(\" auc is \",auc(fpr, tpr))\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m100",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m100"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
